{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b71fc184",
   "metadata": {},
   "source": [
    "# Gradient Descent for Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a625ad",
   "metadata": {},
   "source": [
    "## Import Data and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7580868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6dcfe0",
   "metadata": {},
   "source": [
    "In this tutorial, we will try to build a simple linear regression model from scratch to predict `Sales` as *dependent variable* (*target*) using `TV Price` as *independent variable* (*predictor*)\n",
    "\n",
    "Recall that Univariate Linear Regression is just a simple linear equation $ f(x) = w.x + b $. The idea of univariate linear regression is as simple as finding the approximate function $ f(x) $ which can minimize the error between *target* and *prediction*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dd860fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/tvmarketing.csv\")\n",
    "X = data.TV.values\n",
    "y = data.Sales.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56ee86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Sales\n",
       "0  230.1   22.1\n",
       "1   44.5   10.4\n",
       "2   17.2    9.3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f4c7d6",
   "metadata": {},
   "source": [
    "In order to make the computation much faster, we can scale the `TV` column. In this tutorial, I used a StandardScaler from scikit-learn. This scaling process sometimes called as `z-score standardization` which we can see as follow:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    $ x_{scale} = \\frac{x^{i} - \\mu}{\\sigma}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b6a07da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150.019375]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, 1))\n",
    "print(scaler.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ffc4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0facca",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "> Problem: How to find $w$ and $b$ so $ f(x) $ can minimize the error?\n",
    "\n",
    "To solve this problem, we can use an optimization algorithm which is called as **Gradient Descent**.\n",
    "\n",
    "I won't talk much about this algorithm, but this are the core steps of the algorithm:\n",
    "\n",
    "`Loop`:\n",
    "    \n",
    "1. Initialize $w$ and $b$ with a value\n",
    "\n",
    "\n",
    "2. Compute cost function over all training samples\n",
    "<center>\n",
    "$\n",
    "    \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) = (\\hat{y}^{(i)} - y^{i})^{2} = (f_{w, b}(x^{(i)}) - y^{(i)})^{2} = (w.x^{(i)} + b - y^{(i)})^{2} \\\\\n",
    "    \\mathcal{J}(w, b) = \\frac{1}{2m}\\sum_{i=1} ^{m} \\mathcal{L}(\\hat{y}^{(i)}, y^{(i)}) = \\frac{1}{2m}\\sum_{i=1} ^{m} (w.x^{(i)} + b - y^{(i)})^{2}\n",
    "$\n",
    "</center>\n",
    "    \n",
    "\n",
    "3. Update $w$ and $b$ simultaneously\n",
    "<center>\n",
    "$\n",
    "    w = w - \\alpha\\frac{\\partial{\\mathcal{J}(w, b)}}{\\partial{w}} \\\\\n",
    "    b = b - \\alpha\\frac{\\partial{\\mathcal{J}(w, b)}}{\\partial{b}}\n",
    "$  \n",
    "</center>\n",
    "\n",
    "\n",
    "4. Back to 1st step until the algorithm converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d0cdb7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160,), (160,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "656e5bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    total_cost = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * X[i] + b\n",
    "        cost = (f_wb - y[i]) ** 2\n",
    "        total_cost += cost\n",
    "    return 1 / (2 * m) * total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "946ae284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.82458592927999"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(X_train, y_train, 1.5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a881aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    for i in range(m):\n",
    "        x_i = X[i]\n",
    "        y_i = y[i]\n",
    "        f_i = (w * x_i + b - y_i)\n",
    "        dj_dw_i = (f_i - y_i) * x_i\n",
    "        dj_db_i = (f_i - y_i)\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fb74f249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6.331385427626634, -25.2)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradients(X_train, y_train, w=1.5, b=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "22ffd0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_gradient_descent(X, y, w_init, b_init, learning_rate, compute_gradients=compute_gradients):\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    dj_dw, dj_db = compute_gradients(X, y, w=w_init, b=b_init)\n",
    "    w = w - learning_rate * dj_dw\n",
    "    b = b - learning_rate * dj_db\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "287253a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5633138542762663, 3.252)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_step_gradient_descent(X_train, y_train, 1.5, 3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "47d46161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_init, b_init, num_iters, learning_rate=1e-6, compute_cost=compute_cost):\n",
    "    losses = []\n",
    "    params_hist = []\n",
    "    w = w_init\n",
    "    b = b_init\n",
    "    for iteration in range(num_iters):\n",
    "        loss = compute_cost(X, y, w, b)\n",
    "        if iteration != 0 and loss > losses[iteration - 1]:\n",
    "            print(\"[STOP] The Algorithm has converged\")\n",
    "            break\n",
    "        w, b = one_step_gradient_descent(X, y, w, b, learning_rate)\n",
    "        losses.append(loss)\n",
    "        params_hist.append([w, b])\n",
    "        if iteration % 1000 == 0:\n",
    "            print(f\"Iteration: {iteration:05d} | Loss: {loss:.3f} | W: {w:.3f} | b = {b:.3f}\")\n",
    "    return losses, params_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c25f7d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00000 | Loss: 112.373 | W: 0.000 | b = 0.000\n",
      "Iteration: 01000 | Loss: 111.945 | W: 0.008 | b = 0.028\n",
      "Iteration: 02000 | Loss: 111.519 | W: 0.016 | b = 0.056\n",
      "Iteration: 03000 | Loss: 111.094 | W: 0.023 | b = 0.085\n",
      "Iteration: 04000 | Loss: 110.670 | W: 0.031 | b = 0.113\n",
      "Iteration: 05000 | Loss: 110.248 | W: 0.039 | b = 0.141\n",
      "Iteration: 06000 | Loss: 109.826 | W: 0.047 | b = 0.169\n",
      "Iteration: 07000 | Loss: 109.406 | W: 0.055 | b = 0.197\n",
      "Iteration: 08000 | Loss: 108.988 | W: 0.062 | b = 0.225\n",
      "Iteration: 09000 | Loss: 108.570 | W: 0.070 | b = 0.253\n",
      "Iteration: 10000 | Loss: 108.154 | W: 0.078 | b = 0.281\n",
      "Iteration: 11000 | Loss: 107.739 | W: 0.086 | b = 0.309\n",
      "Iteration: 12000 | Loss: 107.325 | W: 0.093 | b = 0.336\n",
      "Iteration: 13000 | Loss: 106.913 | W: 0.101 | b = 0.364\n",
      "Iteration: 14000 | Loss: 106.502 | W: 0.109 | b = 0.392\n",
      "Iteration: 15000 | Loss: 106.092 | W: 0.117 | b = 0.420\n",
      "Iteration: 16000 | Loss: 105.683 | W: 0.124 | b = 0.448\n",
      "Iteration: 17000 | Loss: 105.276 | W: 0.132 | b = 0.475\n",
      "Iteration: 18000 | Loss: 104.869 | W: 0.140 | b = 0.503\n",
      "Iteration: 19000 | Loss: 104.464 | W: 0.147 | b = 0.531\n",
      "Iteration: 20000 | Loss: 104.060 | W: 0.155 | b = 0.558\n",
      "Iteration: 21000 | Loss: 103.658 | W: 0.163 | b = 0.586\n",
      "Iteration: 22000 | Loss: 103.257 | W: 0.170 | b = 0.614\n",
      "Iteration: 23000 | Loss: 102.856 | W: 0.178 | b = 0.641\n",
      "Iteration: 24000 | Loss: 102.457 | W: 0.186 | b = 0.669\n",
      "Iteration: 25000 | Loss: 102.060 | W: 0.193 | b = 0.696\n",
      "Iteration: 26000 | Loss: 101.663 | W: 0.201 | b = 0.724\n",
      "Iteration: 27000 | Loss: 101.268 | W: 0.209 | b = 0.751\n",
      "Iteration: 28000 | Loss: 100.874 | W: 0.216 | b = 0.779\n",
      "Iteration: 29000 | Loss: 100.481 | W: 0.224 | b = 0.806\n",
      "Iteration: 30000 | Loss: 100.089 | W: 0.231 | b = 0.833\n",
      "Iteration: 31000 | Loss: 99.699 | W: 0.239 | b = 0.861\n",
      "Iteration: 32000 | Loss: 99.310 | W: 0.247 | b = 0.888\n",
      "Iteration: 33000 | Loss: 98.922 | W: 0.254 | b = 0.915\n",
      "Iteration: 34000 | Loss: 98.535 | W: 0.262 | b = 0.943\n",
      "Iteration: 35000 | Loss: 98.149 | W: 0.269 | b = 0.970\n",
      "Iteration: 36000 | Loss: 97.765 | W: 0.277 | b = 0.997\n",
      "Iteration: 37000 | Loss: 97.381 | W: 0.284 | b = 1.024\n",
      "Iteration: 38000 | Loss: 96.999 | W: 0.292 | b = 1.052\n",
      "Iteration: 39000 | Loss: 96.618 | W: 0.300 | b = 1.079\n",
      "Iteration: 40000 | Loss: 96.238 | W: 0.307 | b = 1.106\n",
      "Iteration: 41000 | Loss: 95.860 | W: 0.315 | b = 1.133\n",
      "Iteration: 42000 | Loss: 95.482 | W: 0.322 | b = 1.160\n",
      "Iteration: 43000 | Loss: 95.106 | W: 0.330 | b = 1.187\n",
      "Iteration: 44000 | Loss: 94.731 | W: 0.337 | b = 1.214\n",
      "Iteration: 45000 | Loss: 94.357 | W: 0.345 | b = 1.241\n",
      "Iteration: 46000 | Loss: 93.984 | W: 0.352 | b = 1.268\n",
      "Iteration: 47000 | Loss: 93.612 | W: 0.360 | b = 1.295\n",
      "Iteration: 48000 | Loss: 93.242 | W: 0.367 | b = 1.322\n",
      "Iteration: 49000 | Loss: 92.872 | W: 0.374 | b = 1.349\n",
      "Iteration: 50000 | Loss: 92.504 | W: 0.382 | b = 1.375\n",
      "Iteration: 51000 | Loss: 92.137 | W: 0.389 | b = 1.402\n",
      "Iteration: 52000 | Loss: 91.771 | W: 0.397 | b = 1.429\n",
      "Iteration: 53000 | Loss: 91.406 | W: 0.404 | b = 1.456\n",
      "Iteration: 54000 | Loss: 91.043 | W: 0.412 | b = 1.482\n",
      "Iteration: 55000 | Loss: 90.680 | W: 0.419 | b = 1.509\n",
      "Iteration: 56000 | Loss: 90.319 | W: 0.427 | b = 1.536\n",
      "Iteration: 57000 | Loss: 89.958 | W: 0.434 | b = 1.562\n",
      "Iteration: 58000 | Loss: 89.599 | W: 0.441 | b = 1.589\n",
      "Iteration: 59000 | Loss: 89.241 | W: 0.449 | b = 1.616\n",
      "Iteration: 60000 | Loss: 88.884 | W: 0.456 | b = 1.642\n",
      "Iteration: 61000 | Loss: 88.528 | W: 0.463 | b = 1.669\n",
      "Iteration: 62000 | Loss: 88.174 | W: 0.471 | b = 1.695\n",
      "Iteration: 63000 | Loss: 87.820 | W: 0.478 | b = 1.722\n",
      "Iteration: 64000 | Loss: 87.468 | W: 0.486 | b = 1.748\n",
      "Iteration: 65000 | Loss: 87.116 | W: 0.493 | b = 1.775\n",
      "Iteration: 66000 | Loss: 86.766 | W: 0.500 | b = 1.801\n",
      "Iteration: 67000 | Loss: 86.417 | W: 0.508 | b = 1.828\n",
      "Iteration: 68000 | Loss: 86.069 | W: 0.515 | b = 1.854\n",
      "Iteration: 69000 | Loss: 85.722 | W: 0.522 | b = 1.880\n",
      "Iteration: 70000 | Loss: 85.376 | W: 0.529 | b = 1.907\n",
      "Iteration: 71000 | Loss: 85.031 | W: 0.537 | b = 1.933\n",
      "Iteration: 72000 | Loss: 84.687 | W: 0.544 | b = 1.959\n",
      "Iteration: 73000 | Loss: 84.345 | W: 0.551 | b = 1.985\n",
      "Iteration: 74000 | Loss: 84.003 | W: 0.559 | b = 2.011\n",
      "Iteration: 75000 | Loss: 83.663 | W: 0.566 | b = 2.038\n",
      "Iteration: 76000 | Loss: 83.323 | W: 0.573 | b = 2.064\n",
      "Iteration: 77000 | Loss: 82.985 | W: 0.580 | b = 2.090\n",
      "Iteration: 78000 | Loss: 82.648 | W: 0.588 | b = 2.116\n",
      "Iteration: 79000 | Loss: 82.312 | W: 0.595 | b = 2.142\n",
      "Iteration: 80000 | Loss: 81.977 | W: 0.602 | b = 2.168\n",
      "Iteration: 81000 | Loss: 81.643 | W: 0.609 | b = 2.194\n",
      "Iteration: 82000 | Loss: 81.310 | W: 0.617 | b = 2.220\n",
      "Iteration: 83000 | Loss: 80.978 | W: 0.624 | b = 2.246\n",
      "Iteration: 84000 | Loss: 80.647 | W: 0.631 | b = 2.272\n",
      "Iteration: 85000 | Loss: 80.317 | W: 0.638 | b = 2.298\n",
      "Iteration: 86000 | Loss: 79.988 | W: 0.645 | b = 2.324\n",
      "Iteration: 87000 | Loss: 79.661 | W: 0.653 | b = 2.350\n",
      "Iteration: 88000 | Loss: 79.334 | W: 0.660 | b = 2.376\n",
      "Iteration: 89000 | Loss: 79.008 | W: 0.667 | b = 2.401\n",
      "Iteration: 90000 | Loss: 78.684 | W: 0.674 | b = 2.427\n",
      "Iteration: 91000 | Loss: 78.360 | W: 0.681 | b = 2.453\n",
      "Iteration: 92000 | Loss: 78.038 | W: 0.688 | b = 2.479\n",
      "Iteration: 93000 | Loss: 77.716 | W: 0.695 | b = 2.504\n",
      "Iteration: 94000 | Loss: 77.396 | W: 0.703 | b = 2.530\n",
      "Iteration: 95000 | Loss: 77.076 | W: 0.710 | b = 2.556\n",
      "Iteration: 96000 | Loss: 76.758 | W: 0.717 | b = 2.581\n",
      "Iteration: 97000 | Loss: 76.441 | W: 0.724 | b = 2.607\n",
      "Iteration: 98000 | Loss: 76.124 | W: 0.731 | b = 2.633\n",
      "Iteration: 99000 | Loss: 75.809 | W: 0.738 | b = 2.658\n",
      "Iteration: 100000 | Loss: 75.495 | W: 0.745 | b = 2.684\n",
      "Iteration: 101000 | Loss: 75.182 | W: 0.752 | b = 2.709\n",
      "Iteration: 102000 | Loss: 74.869 | W: 0.759 | b = 2.735\n",
      "Iteration: 103000 | Loss: 74.558 | W: 0.766 | b = 2.760\n",
      "Iteration: 104000 | Loss: 74.248 | W: 0.774 | b = 2.785\n",
      "Iteration: 105000 | Loss: 73.939 | W: 0.781 | b = 2.811\n",
      "Iteration: 106000 | Loss: 73.630 | W: 0.788 | b = 2.836\n",
      "Iteration: 107000 | Loss: 73.323 | W: 0.795 | b = 2.862\n",
      "Iteration: 108000 | Loss: 73.017 | W: 0.802 | b = 2.887\n",
      "Iteration: 109000 | Loss: 72.712 | W: 0.809 | b = 2.912\n",
      "Iteration: 110000 | Loss: 72.408 | W: 0.816 | b = 2.938\n",
      "Iteration: 111000 | Loss: 72.104 | W: 0.823 | b = 2.963\n",
      "Iteration: 112000 | Loss: 71.802 | W: 0.830 | b = 2.988\n",
      "Iteration: 113000 | Loss: 71.501 | W: 0.837 | b = 3.013\n",
      "Iteration: 114000 | Loss: 71.200 | W: 0.844 | b = 3.038\n",
      "Iteration: 115000 | Loss: 70.901 | W: 0.851 | b = 3.064\n",
      "Iteration: 116000 | Loss: 70.603 | W: 0.858 | b = 3.089\n",
      "Iteration: 117000 | Loss: 70.305 | W: 0.865 | b = 3.114\n",
      "Iteration: 118000 | Loss: 70.009 | W: 0.872 | b = 3.139\n",
      "Iteration: 119000 | Loss: 69.714 | W: 0.879 | b = 3.164\n",
      "Iteration: 120000 | Loss: 69.419 | W: 0.886 | b = 3.189\n",
      "Iteration: 121000 | Loss: 69.126 | W: 0.893 | b = 3.214\n",
      "Iteration: 122000 | Loss: 68.833 | W: 0.899 | b = 3.239\n",
      "Iteration: 123000 | Loss: 68.542 | W: 0.906 | b = 3.264\n",
      "Iteration: 124000 | Loss: 68.251 | W: 0.913 | b = 3.289\n",
      "Iteration: 125000 | Loss: 67.962 | W: 0.920 | b = 3.314\n",
      "Iteration: 126000 | Loss: 67.673 | W: 0.927 | b = 3.338\n",
      "Iteration: 127000 | Loss: 67.385 | W: 0.934 | b = 3.363\n",
      "Iteration: 128000 | Loss: 67.098 | W: 0.941 | b = 3.388\n",
      "Iteration: 129000 | Loss: 66.813 | W: 0.948 | b = 3.413\n",
      "Iteration: 130000 | Loss: 66.528 | W: 0.955 | b = 3.438\n",
      "Iteration: 131000 | Loss: 66.244 | W: 0.962 | b = 3.462\n",
      "Iteration: 132000 | Loss: 65.961 | W: 0.968 | b = 3.487\n",
      "Iteration: 133000 | Loss: 65.679 | W: 0.975 | b = 3.512\n",
      "Iteration: 134000 | Loss: 65.398 | W: 0.982 | b = 3.537\n",
      "Iteration: 135000 | Loss: 65.118 | W: 0.989 | b = 3.561\n",
      "Iteration: 136000 | Loss: 64.838 | W: 0.996 | b = 3.586\n",
      "Iteration: 137000 | Loss: 64.560 | W: 1.003 | b = 3.610\n",
      "Iteration: 138000 | Loss: 64.283 | W: 1.009 | b = 3.635\n",
      "Iteration: 139000 | Loss: 64.006 | W: 1.016 | b = 3.660\n",
      "Iteration: 140000 | Loss: 63.731 | W: 1.023 | b = 3.684\n",
      "Iteration: 141000 | Loss: 63.456 | W: 1.030 | b = 3.709\n",
      "Iteration: 142000 | Loss: 63.183 | W: 1.037 | b = 3.733\n",
      "Iteration: 143000 | Loss: 62.910 | W: 1.044 | b = 3.758\n",
      "Iteration: 144000 | Loss: 62.638 | W: 1.050 | b = 3.782\n",
      "Iteration: 145000 | Loss: 62.367 | W: 1.057 | b = 3.806\n",
      "Iteration: 146000 | Loss: 62.097 | W: 1.064 | b = 3.831\n",
      "Iteration: 147000 | Loss: 61.828 | W: 1.071 | b = 3.855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 148000 | Loss: 61.560 | W: 1.077 | b = 3.879\n",
      "Iteration: 149000 | Loss: 61.292 | W: 1.084 | b = 3.904\n",
      "Iteration: 150000 | Loss: 61.026 | W: 1.091 | b = 3.928\n",
      "Iteration: 151000 | Loss: 60.761 | W: 1.098 | b = 3.952\n",
      "Iteration: 152000 | Loss: 60.496 | W: 1.104 | b = 3.977\n",
      "Iteration: 153000 | Loss: 60.232 | W: 1.111 | b = 4.001\n",
      "Iteration: 154000 | Loss: 59.970 | W: 1.118 | b = 4.025\n",
      "Iteration: 155000 | Loss: 59.708 | W: 1.124 | b = 4.049\n",
      "Iteration: 156000 | Loss: 59.447 | W: 1.131 | b = 4.073\n",
      "Iteration: 157000 | Loss: 59.186 | W: 1.138 | b = 4.097\n",
      "Iteration: 158000 | Loss: 58.927 | W: 1.145 | b = 4.121\n",
      "Iteration: 159000 | Loss: 58.669 | W: 1.151 | b = 4.146\n",
      "Iteration: 160000 | Loss: 58.411 | W: 1.158 | b = 4.170\n",
      "Iteration: 161000 | Loss: 58.155 | W: 1.165 | b = 4.194\n",
      "Iteration: 162000 | Loss: 57.899 | W: 1.171 | b = 4.218\n",
      "Iteration: 163000 | Loss: 57.644 | W: 1.178 | b = 4.242\n",
      "Iteration: 164000 | Loss: 57.390 | W: 1.185 | b = 4.266\n",
      "Iteration: 165000 | Loss: 57.137 | W: 1.191 | b = 4.289\n",
      "Iteration: 166000 | Loss: 56.885 | W: 1.198 | b = 4.313\n",
      "Iteration: 167000 | Loss: 56.634 | W: 1.204 | b = 4.337\n",
      "Iteration: 168000 | Loss: 56.383 | W: 1.211 | b = 4.361\n",
      "Iteration: 169000 | Loss: 56.133 | W: 1.218 | b = 4.385\n",
      "Iteration: 170000 | Loss: 55.885 | W: 1.224 | b = 4.409\n",
      "Iteration: 171000 | Loss: 55.637 | W: 1.231 | b = 4.432\n",
      "Iteration: 172000 | Loss: 55.390 | W: 1.238 | b = 4.456\n",
      "Iteration: 173000 | Loss: 55.143 | W: 1.244 | b = 4.480\n",
      "Iteration: 174000 | Loss: 54.898 | W: 1.251 | b = 4.504\n",
      "Iteration: 175000 | Loss: 54.654 | W: 1.257 | b = 4.527\n",
      "Iteration: 176000 | Loss: 54.410 | W: 1.264 | b = 4.551\n",
      "Iteration: 177000 | Loss: 54.167 | W: 1.270 | b = 4.575\n",
      "Iteration: 178000 | Loss: 53.925 | W: 1.277 | b = 4.598\n",
      "Iteration: 179000 | Loss: 53.684 | W: 1.284 | b = 4.622\n",
      "Iteration: 180000 | Loss: 53.444 | W: 1.290 | b = 4.645\n",
      "Iteration: 181000 | Loss: 53.204 | W: 1.297 | b = 4.669\n",
      "Iteration: 182000 | Loss: 52.966 | W: 1.303 | b = 4.692\n",
      "Iteration: 183000 | Loss: 52.728 | W: 1.310 | b = 4.716\n",
      "Iteration: 184000 | Loss: 52.491 | W: 1.316 | b = 4.739\n",
      "Iteration: 185000 | Loss: 52.255 | W: 1.323 | b = 4.763\n",
      "Iteration: 186000 | Loss: 52.019 | W: 1.329 | b = 4.786\n",
      "Iteration: 187000 | Loss: 51.785 | W: 1.336 | b = 4.810\n",
      "Iteration: 188000 | Loss: 51.551 | W: 1.342 | b = 4.833\n",
      "Iteration: 189000 | Loss: 51.318 | W: 1.349 | b = 4.856\n",
      "Iteration: 190000 | Loss: 51.086 | W: 1.355 | b = 4.880\n",
      "Iteration: 191000 | Loss: 50.855 | W: 1.362 | b = 4.903\n",
      "Iteration: 192000 | Loss: 50.625 | W: 1.368 | b = 4.926\n",
      "Iteration: 193000 | Loss: 50.395 | W: 1.375 | b = 4.950\n",
      "Iteration: 194000 | Loss: 50.167 | W: 1.381 | b = 4.973\n",
      "Iteration: 195000 | Loss: 49.939 | W: 1.387 | b = 4.996\n",
      "Iteration: 196000 | Loss: 49.711 | W: 1.394 | b = 5.019\n",
      "Iteration: 197000 | Loss: 49.485 | W: 1.400 | b = 5.042\n",
      "Iteration: 198000 | Loss: 49.260 | W: 1.407 | b = 5.066\n",
      "Iteration: 199000 | Loss: 49.035 | W: 1.413 | b = 5.089\n",
      "Iteration: 200000 | Loss: 48.811 | W: 1.420 | b = 5.112\n",
      "Iteration: 201000 | Loss: 48.588 | W: 1.426 | b = 5.135\n",
      "Iteration: 202000 | Loss: 48.366 | W: 1.432 | b = 5.158\n",
      "Iteration: 203000 | Loss: 48.144 | W: 1.439 | b = 5.181\n",
      "Iteration: 204000 | Loss: 47.923 | W: 1.445 | b = 5.204\n",
      "Iteration: 205000 | Loss: 47.703 | W: 1.452 | b = 5.227\n",
      "Iteration: 206000 | Loss: 47.484 | W: 1.458 | b = 5.250\n",
      "Iteration: 207000 | Loss: 47.266 | W: 1.464 | b = 5.273\n",
      "Iteration: 208000 | Loss: 47.048 | W: 1.471 | b = 5.296\n",
      "Iteration: 209000 | Loss: 46.831 | W: 1.477 | b = 5.319\n",
      "Iteration: 210000 | Loss: 46.615 | W: 1.483 | b = 5.342\n",
      "Iteration: 211000 | Loss: 46.400 | W: 1.490 | b = 5.364\n",
      "Iteration: 212000 | Loss: 46.186 | W: 1.496 | b = 5.387\n",
      "Iteration: 213000 | Loss: 45.972 | W: 1.502 | b = 5.410\n",
      "Iteration: 214000 | Loss: 45.759 | W: 1.509 | b = 5.433\n",
      "Iteration: 215000 | Loss: 45.547 | W: 1.515 | b = 5.456\n",
      "Iteration: 216000 | Loss: 45.335 | W: 1.521 | b = 5.478\n",
      "Iteration: 217000 | Loss: 45.125 | W: 1.528 | b = 5.501\n",
      "Iteration: 218000 | Loss: 44.915 | W: 1.534 | b = 5.524\n",
      "Iteration: 219000 | Loss: 44.706 | W: 1.540 | b = 5.546\n",
      "Iteration: 220000 | Loss: 44.497 | W: 1.547 | b = 5.569\n",
      "Iteration: 221000 | Loss: 44.290 | W: 1.553 | b = 5.592\n",
      "Iteration: 222000 | Loss: 44.083 | W: 1.559 | b = 5.614\n",
      "Iteration: 223000 | Loss: 43.877 | W: 1.565 | b = 5.637\n",
      "Iteration: 224000 | Loss: 43.672 | W: 1.572 | b = 5.659\n",
      "Iteration: 225000 | Loss: 43.467 | W: 1.578 | b = 5.682\n",
      "Iteration: 226000 | Loss: 43.263 | W: 1.584 | b = 5.704\n",
      "Iteration: 227000 | Loss: 43.060 | W: 1.590 | b = 5.727\n",
      "Iteration: 228000 | Loss: 42.858 | W: 1.597 | b = 5.749\n",
      "Iteration: 229000 | Loss: 42.656 | W: 1.603 | b = 5.772\n",
      "Iteration: 230000 | Loss: 42.456 | W: 1.609 | b = 5.794\n",
      "Iteration: 231000 | Loss: 42.255 | W: 1.615 | b = 5.817\n",
      "Iteration: 232000 | Loss: 42.056 | W: 1.622 | b = 5.839\n",
      "Iteration: 233000 | Loss: 41.858 | W: 1.628 | b = 5.861\n",
      "Iteration: 234000 | Loss: 41.660 | W: 1.634 | b = 5.884\n",
      "Iteration: 235000 | Loss: 41.463 | W: 1.640 | b = 5.906\n",
      "Iteration: 236000 | Loss: 41.266 | W: 1.646 | b = 5.928\n",
      "Iteration: 237000 | Loss: 41.070 | W: 1.652 | b = 5.950\n",
      "Iteration: 238000 | Loss: 40.875 | W: 1.659 | b = 5.973\n",
      "Iteration: 239000 | Loss: 40.681 | W: 1.665 | b = 5.995\n",
      "Iteration: 240000 | Loss: 40.488 | W: 1.671 | b = 6.017\n",
      "Iteration: 241000 | Loss: 40.295 | W: 1.677 | b = 6.039\n",
      "Iteration: 242000 | Loss: 40.103 | W: 1.683 | b = 6.061\n",
      "Iteration: 243000 | Loss: 39.912 | W: 1.689 | b = 6.084\n",
      "Iteration: 244000 | Loss: 39.721 | W: 1.696 | b = 6.106\n",
      "Iteration: 245000 | Loss: 39.531 | W: 1.702 | b = 6.128\n",
      "Iteration: 246000 | Loss: 39.342 | W: 1.708 | b = 6.150\n",
      "Iteration: 247000 | Loss: 39.153 | W: 1.714 | b = 6.172\n",
      "Iteration: 248000 | Loss: 38.966 | W: 1.720 | b = 6.194\n",
      "Iteration: 249000 | Loss: 38.779 | W: 1.726 | b = 6.216\n",
      "Iteration: 250000 | Loss: 38.592 | W: 1.732 | b = 6.238\n",
      "Iteration: 251000 | Loss: 38.407 | W: 1.738 | b = 6.260\n",
      "Iteration: 252000 | Loss: 38.222 | W: 1.744 | b = 6.282\n",
      "Iteration: 253000 | Loss: 38.038 | W: 1.751 | b = 6.304\n",
      "Iteration: 254000 | Loss: 37.854 | W: 1.757 | b = 6.326\n",
      "Iteration: 255000 | Loss: 37.671 | W: 1.763 | b = 6.347\n",
      "Iteration: 256000 | Loss: 37.489 | W: 1.769 | b = 6.369\n",
      "Iteration: 257000 | Loss: 37.308 | W: 1.775 | b = 6.391\n",
      "Iteration: 258000 | Loss: 37.127 | W: 1.781 | b = 6.413\n",
      "Iteration: 259000 | Loss: 36.947 | W: 1.787 | b = 6.435\n",
      "Iteration: 260000 | Loss: 36.767 | W: 1.793 | b = 6.456\n",
      "Iteration: 261000 | Loss: 36.589 | W: 1.799 | b = 6.478\n",
      "Iteration: 262000 | Loss: 36.411 | W: 1.805 | b = 6.500\n",
      "Iteration: 263000 | Loss: 36.233 | W: 1.811 | b = 6.522\n",
      "Iteration: 264000 | Loss: 36.057 | W: 1.817 | b = 6.543\n",
      "Iteration: 265000 | Loss: 35.881 | W: 1.823 | b = 6.565\n",
      "Iteration: 266000 | Loss: 35.706 | W: 1.829 | b = 6.586\n",
      "Iteration: 267000 | Loss: 35.531 | W: 1.835 | b = 6.608\n",
      "Iteration: 268000 | Loss: 35.357 | W: 1.841 | b = 6.630\n",
      "Iteration: 269000 | Loss: 35.184 | W: 1.847 | b = 6.651\n",
      "Iteration: 270000 | Loss: 35.011 | W: 1.853 | b = 6.673\n",
      "Iteration: 271000 | Loss: 34.840 | W: 1.859 | b = 6.694\n",
      "Iteration: 272000 | Loss: 34.668 | W: 1.865 | b = 6.716\n",
      "Iteration: 273000 | Loss: 34.498 | W: 1.871 | b = 6.737\n",
      "Iteration: 274000 | Loss: 34.328 | W: 1.877 | b = 6.759\n",
      "Iteration: 275000 | Loss: 34.159 | W: 1.883 | b = 6.780\n",
      "Iteration: 276000 | Loss: 33.990 | W: 1.889 | b = 6.801\n",
      "Iteration: 277000 | Loss: 33.822 | W: 1.895 | b = 6.823\n",
      "Iteration: 278000 | Loss: 33.655 | W: 1.901 | b = 6.844\n",
      "Iteration: 279000 | Loss: 33.488 | W: 1.907 | b = 6.866\n",
      "Iteration: 280000 | Loss: 33.322 | W: 1.913 | b = 6.887\n",
      "Iteration: 281000 | Loss: 33.157 | W: 1.918 | b = 6.908\n",
      "Iteration: 282000 | Loss: 32.993 | W: 1.924 | b = 6.930\n",
      "Iteration: 283000 | Loss: 32.829 | W: 1.930 | b = 6.951\n",
      "Iteration: 284000 | Loss: 32.665 | W: 1.936 | b = 6.972\n",
      "Iteration: 285000 | Loss: 32.503 | W: 1.942 | b = 6.993\n",
      "Iteration: 286000 | Loss: 32.341 | W: 1.948 | b = 7.014\n",
      "Iteration: 287000 | Loss: 32.179 | W: 1.954 | b = 7.036\n",
      "Iteration: 288000 | Loss: 32.019 | W: 1.960 | b = 7.057\n",
      "Iteration: 289000 | Loss: 31.858 | W: 1.966 | b = 7.078\n",
      "Iteration: 290000 | Loss: 31.699 | W: 1.971 | b = 7.099\n",
      "Iteration: 291000 | Loss: 31.540 | W: 1.977 | b = 7.120\n",
      "Iteration: 292000 | Loss: 31.382 | W: 1.983 | b = 7.141\n",
      "Iteration: 293000 | Loss: 31.225 | W: 1.989 | b = 7.162\n",
      "Iteration: 294000 | Loss: 31.068 | W: 1.995 | b = 7.183\n",
      "Iteration: 295000 | Loss: 30.911 | W: 2.001 | b = 7.204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 296000 | Loss: 30.756 | W: 2.007 | b = 7.225\n",
      "Iteration: 297000 | Loss: 30.601 | W: 2.012 | b = 7.246\n",
      "Iteration: 298000 | Loss: 30.446 | W: 2.018 | b = 7.267\n",
      "Iteration: 299000 | Loss: 30.293 | W: 2.024 | b = 7.288\n",
      "Iteration: 300000 | Loss: 30.140 | W: 2.030 | b = 7.309\n",
      "Iteration: 301000 | Loss: 29.987 | W: 2.036 | b = 7.330\n",
      "Iteration: 302000 | Loss: 29.835 | W: 2.041 | b = 7.351\n",
      "Iteration: 303000 | Loss: 29.684 | W: 2.047 | b = 7.372\n",
      "Iteration: 304000 | Loss: 29.533 | W: 2.053 | b = 7.392\n",
      "Iteration: 305000 | Loss: 29.383 | W: 2.059 | b = 7.413\n",
      "Iteration: 306000 | Loss: 29.234 | W: 2.064 | b = 7.434\n",
      "Iteration: 307000 | Loss: 29.085 | W: 2.070 | b = 7.455\n",
      "Iteration: 308000 | Loss: 28.937 | W: 2.076 | b = 7.475\n",
      "Iteration: 309000 | Loss: 28.789 | W: 2.082 | b = 7.496\n",
      "Iteration: 310000 | Loss: 28.642 | W: 2.087 | b = 7.517\n",
      "Iteration: 311000 | Loss: 28.496 | W: 2.093 | b = 7.537\n",
      "Iteration: 312000 | Loss: 28.350 | W: 2.099 | b = 7.558\n",
      "Iteration: 313000 | Loss: 28.205 | W: 2.105 | b = 7.579\n",
      "Iteration: 314000 | Loss: 28.060 | W: 2.110 | b = 7.599\n",
      "Iteration: 315000 | Loss: 27.916 | W: 2.116 | b = 7.620\n",
      "Iteration: 316000 | Loss: 27.773 | W: 2.122 | b = 7.641\n",
      "Iteration: 317000 | Loss: 27.630 | W: 2.128 | b = 7.661\n",
      "Iteration: 318000 | Loss: 27.488 | W: 2.133 | b = 7.682\n",
      "Iteration: 319000 | Loss: 27.347 | W: 2.139 | b = 7.702\n",
      "Iteration: 320000 | Loss: 27.206 | W: 2.145 | b = 7.723\n",
      "Iteration: 321000 | Loss: 27.065 | W: 2.150 | b = 7.743\n",
      "Iteration: 322000 | Loss: 26.926 | W: 2.156 | b = 7.764\n",
      "Iteration: 323000 | Loss: 26.786 | W: 2.162 | b = 7.784\n",
      "Iteration: 324000 | Loss: 26.648 | W: 2.167 | b = 7.804\n",
      "Iteration: 325000 | Loss: 26.510 | W: 2.173 | b = 7.825\n",
      "Iteration: 326000 | Loss: 26.372 | W: 2.179 | b = 7.845\n",
      "Iteration: 327000 | Loss: 26.236 | W: 2.184 | b = 7.865\n",
      "Iteration: 328000 | Loss: 26.099 | W: 2.190 | b = 7.886\n",
      "Iteration: 329000 | Loss: 25.964 | W: 2.196 | b = 7.906\n",
      "Iteration: 330000 | Loss: 25.828 | W: 2.201 | b = 7.926\n",
      "Iteration: 331000 | Loss: 25.694 | W: 2.207 | b = 7.947\n",
      "Iteration: 332000 | Loss: 25.560 | W: 2.212 | b = 7.967\n",
      "Iteration: 333000 | Loss: 25.427 | W: 2.218 | b = 7.987\n",
      "Iteration: 334000 | Loss: 25.294 | W: 2.224 | b = 8.007\n",
      "Iteration: 335000 | Loss: 25.162 | W: 2.229 | b = 8.027\n",
      "Iteration: 336000 | Loss: 25.030 | W: 2.235 | b = 8.048\n",
      "Iteration: 337000 | Loss: 24.899 | W: 2.240 | b = 8.068\n",
      "Iteration: 338000 | Loss: 24.768 | W: 2.246 | b = 8.088\n",
      "Iteration: 339000 | Loss: 24.638 | W: 2.252 | b = 8.108\n",
      "Iteration: 340000 | Loss: 24.509 | W: 2.257 | b = 8.128\n",
      "Iteration: 341000 | Loss: 24.380 | W: 2.263 | b = 8.148\n",
      "Iteration: 342000 | Loss: 24.252 | W: 2.268 | b = 8.168\n",
      "Iteration: 343000 | Loss: 24.124 | W: 2.274 | b = 8.188\n",
      "Iteration: 344000 | Loss: 23.997 | W: 2.279 | b = 8.208\n",
      "Iteration: 345000 | Loss: 23.870 | W: 2.285 | b = 8.228\n",
      "Iteration: 346000 | Loss: 23.744 | W: 2.291 | b = 8.248\n",
      "Iteration: 347000 | Loss: 23.619 | W: 2.296 | b = 8.268\n",
      "Iteration: 348000 | Loss: 23.494 | W: 2.302 | b = 8.288\n",
      "Iteration: 349000 | Loss: 23.370 | W: 2.307 | b = 8.308\n",
      "Iteration: 350000 | Loss: 23.246 | W: 2.313 | b = 8.328\n",
      "Iteration: 351000 | Loss: 23.122 | W: 2.318 | b = 8.348\n",
      "Iteration: 352000 | Loss: 23.000 | W: 2.324 | b = 8.368\n",
      "Iteration: 353000 | Loss: 22.878 | W: 2.329 | b = 8.387\n",
      "Iteration: 354000 | Loss: 22.756 | W: 2.335 | b = 8.407\n",
      "Iteration: 355000 | Loss: 22.635 | W: 2.340 | b = 8.427\n",
      "Iteration: 356000 | Loss: 22.514 | W: 2.346 | b = 8.447\n",
      "Iteration: 357000 | Loss: 22.394 | W: 2.351 | b = 8.466\n",
      "Iteration: 358000 | Loss: 22.275 | W: 2.357 | b = 8.486\n",
      "Iteration: 359000 | Loss: 22.156 | W: 2.362 | b = 8.506\n",
      "Iteration: 360000 | Loss: 22.037 | W: 2.368 | b = 8.526\n",
      "Iteration: 361000 | Loss: 21.920 | W: 2.373 | b = 8.545\n",
      "Iteration: 362000 | Loss: 21.802 | W: 2.379 | b = 8.565\n",
      "Iteration: 363000 | Loss: 21.685 | W: 2.384 | b = 8.584\n",
      "Iteration: 364000 | Loss: 21.569 | W: 2.389 | b = 8.604\n",
      "Iteration: 365000 | Loss: 21.453 | W: 2.395 | b = 8.624\n",
      "Iteration: 366000 | Loss: 21.338 | W: 2.400 | b = 8.643\n",
      "Iteration: 367000 | Loss: 21.224 | W: 2.406 | b = 8.663\n",
      "Iteration: 368000 | Loss: 21.109 | W: 2.411 | b = 8.682\n",
      "Iteration: 369000 | Loss: 20.996 | W: 2.417 | b = 8.702\n",
      "Iteration: 370000 | Loss: 20.883 | W: 2.422 | b = 8.721\n",
      "Iteration: 371000 | Loss: 20.770 | W: 2.427 | b = 8.741\n",
      "Iteration: 372000 | Loss: 20.658 | W: 2.433 | b = 8.760\n",
      "Iteration: 373000 | Loss: 20.546 | W: 2.438 | b = 8.780\n",
      "Iteration: 374000 | Loss: 20.435 | W: 2.444 | b = 8.799\n",
      "Iteration: 375000 | Loss: 20.325 | W: 2.449 | b = 8.818\n",
      "Iteration: 376000 | Loss: 20.215 | W: 2.454 | b = 8.838\n",
      "Iteration: 377000 | Loss: 20.105 | W: 2.460 | b = 8.857\n",
      "Iteration: 378000 | Loss: 19.996 | W: 2.465 | b = 8.877\n",
      "Iteration: 379000 | Loss: 19.888 | W: 2.470 | b = 8.896\n",
      "Iteration: 380000 | Loss: 19.780 | W: 2.476 | b = 8.915\n",
      "Iteration: 381000 | Loss: 19.673 | W: 2.481 | b = 8.934\n",
      "Iteration: 382000 | Loss: 19.566 | W: 2.487 | b = 8.954\n",
      "Iteration: 383000 | Loss: 19.459 | W: 2.492 | b = 8.973\n",
      "Iteration: 384000 | Loss: 19.353 | W: 2.497 | b = 8.992\n",
      "Iteration: 385000 | Loss: 19.248 | W: 2.503 | b = 9.011\n",
      "Iteration: 386000 | Loss: 19.143 | W: 2.508 | b = 9.030\n",
      "Iteration: 387000 | Loss: 19.038 | W: 2.513 | b = 9.050\n",
      "Iteration: 388000 | Loss: 18.935 | W: 2.518 | b = 9.069\n",
      "Iteration: 389000 | Loss: 18.831 | W: 2.524 | b = 9.088\n",
      "Iteration: 390000 | Loss: 18.728 | W: 2.529 | b = 9.107\n",
      "Iteration: 391000 | Loss: 18.626 | W: 2.534 | b = 9.126\n",
      "Iteration: 392000 | Loss: 18.524 | W: 2.540 | b = 9.145\n",
      "Iteration: 393000 | Loss: 18.422 | W: 2.545 | b = 9.164\n",
      "Iteration: 394000 | Loss: 18.321 | W: 2.550 | b = 9.183\n",
      "Iteration: 395000 | Loss: 18.221 | W: 2.556 | b = 9.202\n",
      "Iteration: 396000 | Loss: 18.121 | W: 2.561 | b = 9.221\n",
      "Iteration: 397000 | Loss: 18.021 | W: 2.566 | b = 9.240\n",
      "Iteration: 398000 | Loss: 17.922 | W: 2.571 | b = 9.259\n",
      "Iteration: 399000 | Loss: 17.824 | W: 2.577 | b = 9.278\n",
      "Iteration: 400000 | Loss: 17.726 | W: 2.582 | b = 9.297\n",
      "Iteration: 401000 | Loss: 17.628 | W: 2.587 | b = 9.316\n",
      "Iteration: 402000 | Loss: 17.531 | W: 2.592 | b = 9.335\n",
      "Iteration: 403000 | Loss: 17.435 | W: 2.598 | b = 9.354\n",
      "Iteration: 404000 | Loss: 17.339 | W: 2.603 | b = 9.372\n",
      "Iteration: 405000 | Loss: 17.243 | W: 2.608 | b = 9.391\n",
      "Iteration: 406000 | Loss: 17.148 | W: 2.613 | b = 9.410\n",
      "Iteration: 407000 | Loss: 17.053 | W: 2.618 | b = 9.429\n",
      "Iteration: 408000 | Loss: 16.959 | W: 2.624 | b = 9.448\n",
      "Iteration: 409000 | Loss: 16.865 | W: 2.629 | b = 9.466\n",
      "Iteration: 410000 | Loss: 16.772 | W: 2.634 | b = 9.485\n",
      "Iteration: 411000 | Loss: 16.679 | W: 2.639 | b = 9.504\n",
      "Iteration: 412000 | Loss: 16.587 | W: 2.644 | b = 9.522\n",
      "Iteration: 413000 | Loss: 16.495 | W: 2.650 | b = 9.541\n",
      "Iteration: 414000 | Loss: 16.403 | W: 2.655 | b = 9.560\n",
      "Iteration: 415000 | Loss: 16.313 | W: 2.660 | b = 9.578\n",
      "Iteration: 416000 | Loss: 16.222 | W: 2.665 | b = 9.597\n",
      "Iteration: 417000 | Loss: 16.132 | W: 2.670 | b = 9.616\n",
      "Iteration: 418000 | Loss: 16.043 | W: 2.676 | b = 9.634\n",
      "Iteration: 419000 | Loss: 15.953 | W: 2.681 | b = 9.653\n",
      "Iteration: 420000 | Loss: 15.865 | W: 2.686 | b = 9.671\n",
      "Iteration: 421000 | Loss: 15.777 | W: 2.691 | b = 9.690\n",
      "Iteration: 422000 | Loss: 15.689 | W: 2.696 | b = 9.708\n",
      "Iteration: 423000 | Loss: 15.602 | W: 2.701 | b = 9.727\n",
      "Iteration: 424000 | Loss: 15.515 | W: 2.706 | b = 9.745\n",
      "Iteration: 425000 | Loss: 15.429 | W: 2.711 | b = 9.764\n",
      "Iteration: 426000 | Loss: 15.343 | W: 2.717 | b = 9.782\n",
      "Iteration: 427000 | Loss: 15.257 | W: 2.722 | b = 9.801\n",
      "Iteration: 428000 | Loss: 15.172 | W: 2.727 | b = 9.819\n",
      "Iteration: 429000 | Loss: 15.088 | W: 2.732 | b = 9.837\n",
      "Iteration: 430000 | Loss: 15.004 | W: 2.737 | b = 9.856\n",
      "Iteration: 431000 | Loss: 14.920 | W: 2.742 | b = 9.874\n",
      "Iteration: 432000 | Loss: 14.837 | W: 2.747 | b = 9.892\n",
      "Iteration: 433000 | Loss: 14.754 | W: 2.752 | b = 9.911\n",
      "Iteration: 434000 | Loss: 14.672 | W: 2.757 | b = 9.929\n",
      "Iteration: 435000 | Loss: 14.590 | W: 2.762 | b = 9.947\n",
      "Iteration: 436000 | Loss: 14.509 | W: 2.767 | b = 9.965\n",
      "Iteration: 437000 | Loss: 14.428 | W: 2.773 | b = 9.984\n",
      "Iteration: 438000 | Loss: 14.347 | W: 2.778 | b = 10.002\n",
      "Iteration: 439000 | Loss: 14.267 | W: 2.783 | b = 10.020\n",
      "Iteration: 440000 | Loss: 14.187 | W: 2.788 | b = 10.038\n",
      "Iteration: 441000 | Loss: 14.108 | W: 2.793 | b = 10.056\n",
      "Iteration: 442000 | Loss: 14.029 | W: 2.798 | b = 10.074\n",
      "Iteration: 443000 | Loss: 13.951 | W: 2.803 | b = 10.093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 444000 | Loss: 13.873 | W: 2.808 | b = 10.111\n",
      "Iteration: 445000 | Loss: 13.795 | W: 2.813 | b = 10.129\n",
      "Iteration: 446000 | Loss: 13.718 | W: 2.818 | b = 10.147\n",
      "Iteration: 447000 | Loss: 13.642 | W: 2.823 | b = 10.165\n",
      "Iteration: 448000 | Loss: 13.565 | W: 2.828 | b = 10.183\n",
      "Iteration: 449000 | Loss: 13.490 | W: 2.833 | b = 10.201\n",
      "Iteration: 450000 | Loss: 13.414 | W: 2.838 | b = 10.219\n",
      "Iteration: 451000 | Loss: 13.339 | W: 2.843 | b = 10.237\n",
      "Iteration: 452000 | Loss: 13.265 | W: 2.848 | b = 10.255\n",
      "Iteration: 453000 | Loss: 13.191 | W: 2.853 | b = 10.273\n",
      "Iteration: 454000 | Loss: 13.117 | W: 2.858 | b = 10.291\n",
      "Iteration: 455000 | Loss: 13.044 | W: 2.863 | b = 10.309\n",
      "Iteration: 456000 | Loss: 12.971 | W: 2.868 | b = 10.326\n",
      "Iteration: 457000 | Loss: 12.898 | W: 2.873 | b = 10.344\n",
      "Iteration: 458000 | Loss: 12.826 | W: 2.878 | b = 10.362\n",
      "Iteration: 459000 | Loss: 12.755 | W: 2.883 | b = 10.380\n",
      "Iteration: 460000 | Loss: 12.683 | W: 2.888 | b = 10.398\n",
      "Iteration: 461000 | Loss: 12.613 | W: 2.893 | b = 10.416\n",
      "Iteration: 462000 | Loss: 12.542 | W: 2.897 | b = 10.433\n",
      "Iteration: 463000 | Loss: 12.472 | W: 2.902 | b = 10.451\n",
      "Iteration: 464000 | Loss: 12.403 | W: 2.907 | b = 10.469\n",
      "Iteration: 465000 | Loss: 12.334 | W: 2.912 | b = 10.487\n",
      "Iteration: 466000 | Loss: 12.265 | W: 2.917 | b = 10.504\n",
      "Iteration: 467000 | Loss: 12.197 | W: 2.922 | b = 10.522\n",
      "Iteration: 468000 | Loss: 12.129 | W: 2.927 | b = 10.540\n",
      "Iteration: 469000 | Loss: 12.061 | W: 2.932 | b = 10.557\n",
      "Iteration: 470000 | Loss: 11.994 | W: 2.937 | b = 10.575\n",
      "Iteration: 471000 | Loss: 11.927 | W: 2.942 | b = 10.593\n",
      "Iteration: 472000 | Loss: 11.861 | W: 2.947 | b = 10.610\n",
      "Iteration: 473000 | Loss: 11.795 | W: 2.951 | b = 10.628\n",
      "Iteration: 474000 | Loss: 11.730 | W: 2.956 | b = 10.645\n",
      "Iteration: 475000 | Loss: 11.664 | W: 2.961 | b = 10.663\n",
      "Iteration: 476000 | Loss: 11.600 | W: 2.966 | b = 10.680\n",
      "Iteration: 477000 | Loss: 11.535 | W: 2.971 | b = 10.698\n",
      "Iteration: 478000 | Loss: 11.471 | W: 2.976 | b = 10.715\n",
      "Iteration: 479000 | Loss: 11.408 | W: 2.981 | b = 10.733\n",
      "Iteration: 480000 | Loss: 11.345 | W: 2.985 | b = 10.750\n",
      "Iteration: 481000 | Loss: 11.282 | W: 2.990 | b = 10.768\n",
      "Iteration: 482000 | Loss: 11.220 | W: 2.995 | b = 10.785\n",
      "Iteration: 483000 | Loss: 11.158 | W: 3.000 | b = 10.803\n",
      "Iteration: 484000 | Loss: 11.096 | W: 3.005 | b = 10.820\n",
      "Iteration: 485000 | Loss: 11.035 | W: 3.010 | b = 10.837\n",
      "Iteration: 486000 | Loss: 10.974 | W: 3.014 | b = 10.855\n",
      "Iteration: 487000 | Loss: 10.913 | W: 3.019 | b = 10.872\n",
      "Iteration: 488000 | Loss: 10.853 | W: 3.024 | b = 10.889\n",
      "Iteration: 489000 | Loss: 10.794 | W: 3.029 | b = 10.907\n",
      "Iteration: 490000 | Loss: 10.734 | W: 3.034 | b = 10.924\n",
      "Iteration: 491000 | Loss: 10.676 | W: 3.038 | b = 10.941\n",
      "Iteration: 492000 | Loss: 10.617 | W: 3.043 | b = 10.958\n",
      "Iteration: 493000 | Loss: 10.559 | W: 3.048 | b = 10.976\n",
      "Iteration: 494000 | Loss: 10.501 | W: 3.053 | b = 10.993\n",
      "Iteration: 495000 | Loss: 10.444 | W: 3.058 | b = 11.010\n",
      "Iteration: 496000 | Loss: 10.387 | W: 3.062 | b = 11.027\n",
      "Iteration: 497000 | Loss: 10.330 | W: 3.067 | b = 11.044\n",
      "Iteration: 498000 | Loss: 10.274 | W: 3.072 | b = 11.062\n",
      "Iteration: 499000 | Loss: 10.218 | W: 3.077 | b = 11.079\n",
      "Iteration: 500000 | Loss: 10.162 | W: 3.081 | b = 11.096\n",
      "Iteration: 501000 | Loss: 10.107 | W: 3.086 | b = 11.113\n",
      "Iteration: 502000 | Loss: 10.052 | W: 3.091 | b = 11.130\n",
      "Iteration: 503000 | Loss: 9.998 | W: 3.096 | b = 11.147\n",
      "Iteration: 504000 | Loss: 9.944 | W: 3.100 | b = 11.164\n",
      "Iteration: 505000 | Loss: 9.890 | W: 3.105 | b = 11.181\n",
      "Iteration: 506000 | Loss: 9.837 | W: 3.110 | b = 11.198\n",
      "Iteration: 507000 | Loss: 9.784 | W: 3.115 | b = 11.215\n",
      "Iteration: 508000 | Loss: 9.731 | W: 3.119 | b = 11.232\n",
      "Iteration: 509000 | Loss: 9.679 | W: 3.124 | b = 11.249\n",
      "Iteration: 510000 | Loss: 9.627 | W: 3.129 | b = 11.266\n",
      "Iteration: 511000 | Loss: 9.576 | W: 3.133 | b = 11.283\n",
      "Iteration: 512000 | Loss: 9.525 | W: 3.138 | b = 11.300\n",
      "Iteration: 513000 | Loss: 9.474 | W: 3.143 | b = 11.317\n",
      "Iteration: 514000 | Loss: 9.423 | W: 3.147 | b = 11.334\n",
      "Iteration: 515000 | Loss: 9.373 | W: 3.152 | b = 11.351\n",
      "Iteration: 516000 | Loss: 9.324 | W: 3.157 | b = 11.367\n",
      "Iteration: 517000 | Loss: 9.274 | W: 3.161 | b = 11.384\n",
      "Iteration: 518000 | Loss: 9.225 | W: 3.166 | b = 11.401\n",
      "Iteration: 519000 | Loss: 9.176 | W: 3.171 | b = 11.418\n",
      "Iteration: 520000 | Loss: 9.128 | W: 3.175 | b = 11.435\n",
      "Iteration: 521000 | Loss: 9.080 | W: 3.180 | b = 11.451\n",
      "Iteration: 522000 | Loss: 9.033 | W: 3.185 | b = 11.468\n",
      "Iteration: 523000 | Loss: 8.985 | W: 3.189 | b = 11.485\n",
      "Iteration: 524000 | Loss: 8.938 | W: 3.194 | b = 11.501\n",
      "Iteration: 525000 | Loss: 8.892 | W: 3.199 | b = 11.518\n",
      "Iteration: 526000 | Loss: 8.846 | W: 3.203 | b = 11.535\n",
      "Iteration: 527000 | Loss: 8.800 | W: 3.208 | b = 11.551\n",
      "Iteration: 528000 | Loss: 8.754 | W: 3.213 | b = 11.568\n",
      "Iteration: 529000 | Loss: 8.709 | W: 3.217 | b = 11.585\n",
      "Iteration: 530000 | Loss: 8.664 | W: 3.222 | b = 11.601\n",
      "Iteration: 531000 | Loss: 8.620 | W: 3.226 | b = 11.618\n",
      "Iteration: 532000 | Loss: 8.576 | W: 3.231 | b = 11.635\n",
      "Iteration: 533000 | Loss: 8.532 | W: 3.236 | b = 11.651\n",
      "Iteration: 534000 | Loss: 8.488 | W: 3.240 | b = 11.668\n",
      "Iteration: 535000 | Loss: 8.445 | W: 3.245 | b = 11.684\n",
      "Iteration: 536000 | Loss: 8.402 | W: 3.249 | b = 11.701\n",
      "Iteration: 537000 | Loss: 8.360 | W: 3.254 | b = 11.717\n",
      "Iteration: 538000 | Loss: 8.318 | W: 3.259 | b = 11.734\n",
      "Iteration: 539000 | Loss: 8.276 | W: 3.263 | b = 11.750\n",
      "Iteration: 540000 | Loss: 8.234 | W: 3.268 | b = 11.767\n",
      "Iteration: 541000 | Loss: 8.193 | W: 3.272 | b = 11.783\n",
      "Iteration: 542000 | Loss: 8.152 | W: 3.277 | b = 11.799\n",
      "Iteration: 543000 | Loss: 8.112 | W: 3.281 | b = 11.816\n",
      "Iteration: 544000 | Loss: 8.072 | W: 3.286 | b = 11.832\n",
      "Iteration: 545000 | Loss: 8.032 | W: 3.290 | b = 11.848\n",
      "Iteration: 546000 | Loss: 7.992 | W: 3.295 | b = 11.865\n",
      "Iteration: 547000 | Loss: 7.953 | W: 3.299 | b = 11.881\n",
      "Iteration: 548000 | Loss: 7.914 | W: 3.304 | b = 11.897\n",
      "Iteration: 549000 | Loss: 7.876 | W: 3.309 | b = 11.914\n",
      "Iteration: 550000 | Loss: 7.838 | W: 3.313 | b = 11.930\n",
      "Iteration: 551000 | Loss: 7.800 | W: 3.318 | b = 11.946\n",
      "Iteration: 552000 | Loss: 7.762 | W: 3.322 | b = 11.963\n",
      "Iteration: 553000 | Loss: 7.725 | W: 3.327 | b = 11.979\n",
      "Iteration: 554000 | Loss: 7.688 | W: 3.331 | b = 11.995\n",
      "Iteration: 555000 | Loss: 7.652 | W: 3.336 | b = 12.011\n",
      "Iteration: 556000 | Loss: 7.615 | W: 3.340 | b = 12.027\n",
      "Iteration: 557000 | Loss: 7.579 | W: 3.345 | b = 12.044\n",
      "Iteration: 558000 | Loss: 7.544 | W: 3.349 | b = 12.060\n",
      "Iteration: 559000 | Loss: 7.509 | W: 3.354 | b = 12.076\n",
      "Iteration: 560000 | Loss: 7.474 | W: 3.358 | b = 12.092\n",
      "Iteration: 561000 | Loss: 7.439 | W: 3.363 | b = 12.108\n",
      "Iteration: 562000 | Loss: 7.404 | W: 3.367 | b = 12.124\n",
      "Iteration: 563000 | Loss: 7.370 | W: 3.371 | b = 12.140\n",
      "Iteration: 564000 | Loss: 7.337 | W: 3.376 | b = 12.156\n",
      "Iteration: 565000 | Loss: 7.303 | W: 3.380 | b = 12.172\n",
      "Iteration: 566000 | Loss: 7.270 | W: 3.385 | b = 12.188\n",
      "Iteration: 567000 | Loss: 7.237 | W: 3.389 | b = 12.204\n",
      "Iteration: 568000 | Loss: 7.205 | W: 3.394 | b = 12.220\n",
      "Iteration: 569000 | Loss: 7.173 | W: 3.398 | b = 12.236\n",
      "Iteration: 570000 | Loss: 7.141 | W: 3.403 | b = 12.252\n",
      "Iteration: 571000 | Loss: 7.109 | W: 3.407 | b = 12.268\n",
      "Iteration: 572000 | Loss: 7.078 | W: 3.411 | b = 12.284\n",
      "Iteration: 573000 | Loss: 7.047 | W: 3.416 | b = 12.300\n",
      "Iteration: 574000 | Loss: 7.016 | W: 3.420 | b = 12.316\n",
      "Iteration: 575000 | Loss: 6.986 | W: 3.425 | b = 12.332\n",
      "Iteration: 576000 | Loss: 6.956 | W: 3.429 | b = 12.348\n",
      "Iteration: 577000 | Loss: 6.926 | W: 3.433 | b = 12.363\n",
      "Iteration: 578000 | Loss: 6.896 | W: 3.438 | b = 12.379\n",
      "Iteration: 579000 | Loss: 6.867 | W: 3.442 | b = 12.395\n",
      "Iteration: 580000 | Loss: 6.838 | W: 3.447 | b = 12.411\n",
      "Iteration: 581000 | Loss: 6.810 | W: 3.451 | b = 12.427\n",
      "Iteration: 582000 | Loss: 6.782 | W: 3.455 | b = 12.442\n",
      "Iteration: 583000 | Loss: 6.754 | W: 3.460 | b = 12.458\n",
      "Iteration: 584000 | Loss: 6.726 | W: 3.464 | b = 12.474\n",
      "Iteration: 585000 | Loss: 6.698 | W: 3.468 | b = 12.490\n",
      "Iteration: 586000 | Loss: 6.671 | W: 3.473 | b = 12.505\n",
      "Iteration: 587000 | Loss: 6.645 | W: 3.477 | b = 12.521\n",
      "Iteration: 588000 | Loss: 6.618 | W: 3.482 | b = 12.537\n",
      "Iteration: 589000 | Loss: 6.592 | W: 3.486 | b = 12.552\n",
      "Iteration: 590000 | Loss: 6.566 | W: 3.490 | b = 12.568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 591000 | Loss: 6.540 | W: 3.495 | b = 12.584\n",
      "Iteration: 592000 | Loss: 6.515 | W: 3.499 | b = 12.599\n",
      "Iteration: 593000 | Loss: 6.490 | W: 3.503 | b = 12.615\n",
      "Iteration: 594000 | Loss: 6.465 | W: 3.508 | b = 12.630\n",
      "Iteration: 595000 | Loss: 6.440 | W: 3.512 | b = 12.646\n",
      "Iteration: 596000 | Loss: 6.416 | W: 3.516 | b = 12.662\n",
      "Iteration: 597000 | Loss: 6.392 | W: 3.521 | b = 12.677\n",
      "Iteration: 598000 | Loss: 6.369 | W: 3.525 | b = 12.693\n",
      "Iteration: 599000 | Loss: 6.345 | W: 3.529 | b = 12.708\n",
      "Iteration: 600000 | Loss: 6.322 | W: 3.533 | b = 12.724\n",
      "Iteration: 601000 | Loss: 6.299 | W: 3.538 | b = 12.739\n",
      "Iteration: 602000 | Loss: 6.277 | W: 3.542 | b = 12.754\n",
      "Iteration: 603000 | Loss: 6.255 | W: 3.546 | b = 12.770\n",
      "Iteration: 604000 | Loss: 6.233 | W: 3.551 | b = 12.785\n",
      "Iteration: 605000 | Loss: 6.211 | W: 3.555 | b = 12.801\n",
      "Iteration: 606000 | Loss: 6.190 | W: 3.559 | b = 12.816\n",
      "Iteration: 607000 | Loss: 6.168 | W: 3.563 | b = 12.831\n",
      "Iteration: 608000 | Loss: 6.148 | W: 3.568 | b = 12.847\n",
      "Iteration: 609000 | Loss: 6.127 | W: 3.572 | b = 12.862\n",
      "Iteration: 610000 | Loss: 6.107 | W: 3.576 | b = 12.878\n",
      "Iteration: 611000 | Loss: 6.087 | W: 3.580 | b = 12.893\n",
      "Iteration: 612000 | Loss: 6.067 | W: 3.585 | b = 12.908\n",
      "Iteration: 613000 | Loss: 6.047 | W: 3.589 | b = 12.923\n",
      "Iteration: 614000 | Loss: 6.028 | W: 3.593 | b = 12.939\n",
      "Iteration: 615000 | Loss: 6.009 | W: 3.597 | b = 12.954\n",
      "Iteration: 616000 | Loss: 5.990 | W: 3.602 | b = 12.969\n",
      "Iteration: 617000 | Loss: 5.972 | W: 3.606 | b = 12.984\n",
      "Iteration: 618000 | Loss: 5.954 | W: 3.610 | b = 13.000\n",
      "Iteration: 619000 | Loss: 5.936 | W: 3.614 | b = 13.015\n",
      "Iteration: 620000 | Loss: 5.918 | W: 3.619 | b = 13.030\n",
      "Iteration: 621000 | Loss: 5.901 | W: 3.623 | b = 13.045\n",
      "Iteration: 622000 | Loss: 5.884 | W: 3.627 | b = 13.060\n",
      "Iteration: 623000 | Loss: 5.867 | W: 3.631 | b = 13.075\n",
      "Iteration: 624000 | Loss: 5.851 | W: 3.635 | b = 13.091\n",
      "Iteration: 625000 | Loss: 5.834 | W: 3.640 | b = 13.106\n",
      "Iteration: 626000 | Loss: 5.818 | W: 3.644 | b = 13.121\n",
      "Iteration: 627000 | Loss: 5.802 | W: 3.648 | b = 13.136\n",
      "Iteration: 628000 | Loss: 5.787 | W: 3.652 | b = 13.151\n",
      "Iteration: 629000 | Loss: 5.772 | W: 3.656 | b = 13.166\n",
      "Iteration: 630000 | Loss: 5.757 | W: 3.660 | b = 13.181\n",
      "Iteration: 631000 | Loss: 5.742 | W: 3.665 | b = 13.196\n",
      "Iteration: 632000 | Loss: 5.728 | W: 3.669 | b = 13.211\n",
      "Iteration: 633000 | Loss: 5.713 | W: 3.673 | b = 13.226\n",
      "Iteration: 634000 | Loss: 5.699 | W: 3.677 | b = 13.241\n",
      "Iteration: 635000 | Loss: 5.686 | W: 3.681 | b = 13.256\n",
      "Iteration: 636000 | Loss: 5.672 | W: 3.685 | b = 13.271\n",
      "Iteration: 637000 | Loss: 5.659 | W: 3.690 | b = 13.286\n",
      "Iteration: 638000 | Loss: 5.646 | W: 3.694 | b = 13.301\n",
      "Iteration: 639000 | Loss: 5.633 | W: 3.698 | b = 13.315\n",
      "Iteration: 640000 | Loss: 5.621 | W: 3.702 | b = 13.330\n",
      "Iteration: 641000 | Loss: 5.609 | W: 3.706 | b = 13.345\n",
      "Iteration: 642000 | Loss: 5.597 | W: 3.710 | b = 13.360\n",
      "Iteration: 643000 | Loss: 5.585 | W: 3.714 | b = 13.375\n",
      "Iteration: 644000 | Loss: 5.574 | W: 3.718 | b = 13.390\n",
      "Iteration: 645000 | Loss: 5.562 | W: 3.723 | b = 13.405\n",
      "Iteration: 646000 | Loss: 5.551 | W: 3.727 | b = 13.419\n",
      "Iteration: 647000 | Loss: 5.541 | W: 3.731 | b = 13.434\n",
      "Iteration: 648000 | Loss: 5.530 | W: 3.735 | b = 13.449\n",
      "Iteration: 649000 | Loss: 5.520 | W: 3.739 | b = 13.464\n",
      "Iteration: 650000 | Loss: 5.510 | W: 3.743 | b = 13.478\n",
      "Iteration: 651000 | Loss: 5.500 | W: 3.747 | b = 13.493\n",
      "Iteration: 652000 | Loss: 5.491 | W: 3.751 | b = 13.508\n",
      "Iteration: 653000 | Loss: 5.481 | W: 3.755 | b = 13.522\n",
      "Iteration: 654000 | Loss: 5.472 | W: 3.759 | b = 13.537\n",
      "Iteration: 655000 | Loss: 5.464 | W: 3.763 | b = 13.552\n",
      "Iteration: 656000 | Loss: 5.455 | W: 3.768 | b = 13.566\n",
      "Iteration: 657000 | Loss: 5.447 | W: 3.772 | b = 13.581\n",
      "Iteration: 658000 | Loss: 5.439 | W: 3.776 | b = 13.596\n",
      "Iteration: 659000 | Loss: 5.431 | W: 3.780 | b = 13.610\n",
      "Iteration: 660000 | Loss: 5.423 | W: 3.784 | b = 13.625\n",
      "Iteration: 661000 | Loss: 5.416 | W: 3.788 | b = 13.639\n",
      "Iteration: 662000 | Loss: 5.409 | W: 3.792 | b = 13.654\n",
      "Iteration: 663000 | Loss: 5.402 | W: 3.796 | b = 13.668\n",
      "Iteration: 664000 | Loss: 5.395 | W: 3.800 | b = 13.683\n",
      "Iteration: 665000 | Loss: 5.389 | W: 3.804 | b = 13.698\n",
      "Iteration: 666000 | Loss: 5.383 | W: 3.808 | b = 13.712\n",
      "Iteration: 667000 | Loss: 5.377 | W: 3.812 | b = 13.726\n",
      "Iteration: 668000 | Loss: 5.371 | W: 3.816 | b = 13.741\n",
      "Iteration: 669000 | Loss: 5.366 | W: 3.820 | b = 13.755\n",
      "Iteration: 670000 | Loss: 5.361 | W: 3.824 | b = 13.770\n",
      "Iteration: 671000 | Loss: 5.355 | W: 3.828 | b = 13.784\n",
      "Iteration: 672000 | Loss: 5.351 | W: 3.832 | b = 13.799\n",
      "Iteration: 673000 | Loss: 5.346 | W: 3.836 | b = 13.813\n",
      "Iteration: 674000 | Loss: 5.342 | W: 3.840 | b = 13.827\n",
      "Iteration: 675000 | Loss: 5.338 | W: 3.844 | b = 13.842\n",
      "Iteration: 676000 | Loss: 5.334 | W: 3.848 | b = 13.856\n",
      "Iteration: 677000 | Loss: 5.330 | W: 3.852 | b = 13.870\n",
      "Iteration: 678000 | Loss: 5.327 | W: 3.856 | b = 13.885\n",
      "Iteration: 679000 | Loss: 5.324 | W: 3.860 | b = 13.899\n",
      "Iteration: 680000 | Loss: 5.321 | W: 3.864 | b = 13.913\n",
      "Iteration: 681000 | Loss: 5.318 | W: 3.868 | b = 13.928\n",
      "Iteration: 682000 | Loss: 5.315 | W: 3.872 | b = 13.942\n",
      "Iteration: 683000 | Loss: 5.313 | W: 3.876 | b = 13.956\n",
      "Iteration: 684000 | Loss: 5.311 | W: 3.880 | b = 13.970\n",
      "Iteration: 685000 | Loss: 5.309 | W: 3.884 | b = 13.985\n",
      "Iteration: 686000 | Loss: 5.307 | W: 3.888 | b = 13.999\n",
      "Iteration: 687000 | Loss: 5.306 | W: 3.892 | b = 14.013\n",
      "Iteration: 688000 | Loss: 5.305 | W: 3.895 | b = 14.027\n",
      "Iteration: 689000 | Loss: 5.304 | W: 3.899 | b = 14.041\n",
      "Iteration: 690000 | Loss: 5.303 | W: 3.903 | b = 14.056\n",
      "Iteration: 691000 | Loss: 5.302 | W: 3.907 | b = 14.070\n",
      "Iteration: 692000 | Loss: 5.302 | W: 3.911 | b = 14.084\n",
      "Iteration: 693000 | Loss: 5.302 | W: 3.915 | b = 14.098\n",
      "[STOP] The Algorithm has converged\n"
     ]
    }
   ],
   "source": [
    "losses, params_hist = gradient_descent(X_train, y_train, w_init=0, b_init=0, num_iters=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "79cd61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_final, b_final = params_hist[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "043c2090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.915697279564234, 14.100016440791467)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_final, b_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "bf0dc05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "def evaluate(X_test, y_test, w_final, b_final):\n",
    "    m = X_test.shape[0]\n",
    "    predictions = []\n",
    "    for i in range(m):\n",
    "        pred = w_final * X_test[i] + b_final\n",
    "        predictions.append(pred)\n",
    "    print(\"R2 Score:\", r2_score(y_test, predictions))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9813935e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.15781217,  0.53925283,  1.69783431, -1.64363349,  0.83513672,\n",
       "       -0.89025846,  0.79354661, -1.18851892,  0.86009078,  0.29803023,\n",
       "       -1.40835233, -1.11484502,  1.00387371, -1.71849568, -0.12500054,\n",
       "        0.23980408, -1.69591819,  0.56539519, -0.88788188,  1.03833409,\n",
       "        0.94445928, -0.97700354, -1.33111357,  1.19875306, -0.96274407,\n",
       "       -1.14692882,  0.75433309, -0.12737712, -0.74647553, -1.68284702,\n",
       "        0.59153754, -0.96036749,  0.58084294, -1.58184248,  1.55286309,\n",
       "        1.04784039, -1.20871983,  1.46968288, -0.38998892, -1.45588388])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = scaler.transform(X_test.reshape(-1, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0]))\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a4776",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c5eda96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.676695871172363\n"
     ]
    }
   ],
   "source": [
    "predictions = evaluate(X_test, y_test, w_final=w_final, b_final=b_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf860f5a",
   "metadata": {},
   "source": [
    "Using simple simple gradient descent algorithm, we can reach R2-Score 67%. This means, 67% of our *dependete*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
